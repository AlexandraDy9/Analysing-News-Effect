{
  "_args": [
    [
      {
        "raw": "natural@^0.6.3",
        "scope": null,
        "escapedName": "natural",
        "name": "natural",
        "rawSpec": "^0.6.3",
        "spec": ">=0.6.3 <0.7.0",
        "type": "range"
      },
      "C:\\Users\\nica_\\Desktop\\nodejs"
    ]
  ],
  "_from": "natural@^0.6.3",
  "_hasShrinkwrap": false,
  "_id": "natural@0.6.3",
  "_location": "/natural",
  "_nodeVersion": "9.11.2",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/natural_0.6.3_1545421625862_0.2617208223018366"
  },
  "_npmUser": {
    "name": "hugo-ter-doest",
    "email": "hwl.ter.doest@gmail.com"
  },
  "_npmVersion": "6.1.0",
  "_phantomChildren": {},
  "_requested": {
    "raw": "natural@^0.6.3",
    "scope": null,
    "escapedName": "natural",
    "name": "natural",
    "rawSpec": "^0.6.3",
    "spec": ">=0.6.3 <0.7.0",
    "type": "range"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/natural/-/natural-0.6.3.tgz",
  "_shasum": "62f56db82fbfddcf9e1f5810c5e39135fdbc1355",
  "_shrinkwrap": null,
  "_spec": "natural@^0.6.3",
  "_where": "C:\\Users\\nica_\\Desktop\\nodejs",
  "author": {
    "name": "Chris Umbel",
    "email": "chris@chrisumbel.com"
  },
  "bugs": {
    "url": "https://github.com/NaturalNode/natural/issues"
  },
  "dependencies": {
    "afinn-165": "^1.0.2",
    "apparatus": "^0.0.10",
    "json-stable-stringify": "^1.0.1",
    "sylvester": "^0.0.12",
    "underscore": "^1.3.1"
  },
  "description": "General natural language (tokenizing, stemming (English, Russian, Spanish), part-of-speech tagging, sentiment analysis, classification, inflection, phonetics, tfidf, WordNet, jaro-winkler, Levenshtein distance, Dice's Coefficient) facilities for node.",
  "devDependencies": {
    "jasmine-node": "^1.13.1",
    "proxyquire": "^1.8.0",
    "rimraf": "^2.6.2",
    "sinon": "^1.12.2",
    "uubench": "^0.0.1"
  },
  "directories": {},
  "dist": {
    "integrity": "sha512-78fcEdNN6Y4pv8SOLPDhJTlUG+8IiQzNx0nYpl0k7q00K4ZZuds+wDWfSa6eeiPcSQDncvV44WWGsi70/ZP3+w==",
    "shasum": "62f56db82fbfddcf9e1f5810c5e39135fdbc1355",
    "tarball": "https://registry.npmjs.org/natural/-/natural-0.6.3.tgz",
    "fileCount": 173,
    "unpackedSize": 12258649,
    "npm-signature": "-----BEGIN PGP SIGNATURE-----\r\nVersion: OpenPGP.js v3.0.4\r\nComment: https://openpgpjs.org\r\n\r\nwsFcBAEBCAAQBQJcHUM6CRA9TVsSAnZWagAA4RwQAKHQiXnnmQmypwkmq8cq\n7NcMEAczVzlk0zaVeKV5eQxmgRAAQznVy7uIU/hgjuMhk9INxVgEiyEF3XzM\nIcO7Qs0iyovPdbXft0VSpL8b6jmcqL4XKPX6frMWsE05RoWmu7lF177WaxFQ\nlSz/DK7HzqbnbKw3bytwkuFL27SSUksMtiPL3xWRZvtnbJwSYBfrCuqyqRVE\nX85wQav59s5ZaBXBZ5MSAxpPxgmDmPBqK3lPz+Z4icnaM/z+a5RnNyYMxlAp\nKRcyBCnA5N+Hi96M6c5NmI1oXSTwZaM0QKETtrxd6tGEthKYARg/iDYg1k4z\nwuTfM2/UBzQ6HrOcWUnJdYSvQlAyN3p2P8hIWs06/4RbLkiAf0vS9xcbRvrC\nMBcjOpIDTMbGUMYkSwnU3CumDGeWLLK/+utehjnqncIa1i96lg5J3iThfDhh\nQU4/+P+3HHt24koWijzlybLSEhWqWjoBaETEYMy7c/UdN4eEn/RBXCWmlAiv\nFQzE6v8Ww/u03Pauq5nzCfxNh+LSDgPeBpNhJsOq19XTN1HLuWgChLjRHKnM\nKP14dXyX58H23XLc3Mr4eZ8oRvdRhnKGqZImBjrfFQB+Pn910kPOGlU38FLI\nr4gJSzxtr3zBGx4+ChFTrYi41FWbxs/RN5E849nWXWMPCr9J0+uwYVgA8v4a\noea1\r\n=J7u8\r\n-----END PGP SIGNATURE-----\r\n"
  },
  "engines": {
    "node": ">=0.4.10"
  },
  "gitHead": "7d0d33562b4d8b74c35cb482102f6478f1d96015",
  "homepage": "https://github.com/NaturalNode/natural",
  "keywords": [
    "natural language processing",
    "artifical intelligence",
    "statistics",
    "Porter stemmer",
    "Lancaster stemmer",
    "tokenizer",
    "bigram",
    "trigram",
    "quadgram",
    "ngram",
    "stemmer",
    "bayes",
    "classifier",
    "phonetic",
    "metaphone",
    "inflector",
    "Wordnet",
    "tf-idf",
    "logistic regression",
    "doublemetaphone",
    "double",
    "jaro-winkler distance",
    "levenshtein distance",
    "string distance",
    "part-of-speech tagger",
    "Eric Brill",
    "Brill tagger",
    "sentiment analysis",
    "maximum entropy modelling"
  ],
  "license": "MIT",
  "main": "./lib/natural/index.js",
  "maintainers": [
    {
      "name": "Chris Umbel",
      "email": "chris@chrisumbel.com",
      "url": "http://www.chrisumbel.com"
    },
    {
      "name": "Rob Ellis",
      "email": "rob@silentrob.me"
    },
    {
      "name": "Ken Koch",
      "email": "kkoch986@gmail.com"
    }
  ],
  "name": "natural",
  "optionalDependencies": {},
  "readme": "natural\n=======\n\n[![NPM version](https://img.shields.io/npm/v/natural.svg)](https://www.npmjs.com/package/natural)\n[![Build Status](https://travis-ci.org/NaturalNode/natural.png?branch=master)](https://travis-ci.org/NaturalNode/natural)\n[![Slack](https://slack.bri.im/badge.svg)](https://slack.bri.im)\n\n\"Natural\" is a general natural language facility for nodejs. Tokenizing,\nstemming, classification, phonetics, tf-idf, WordNet, string similarity,\nand some inflections are currently supported.\n\nIt's still in the early stages, so we're very interested in bug reports,\ncontributions and the like.\n\nNote that many algorithms from Rob Ellis's [node-nltools](https://github.com/NaturalNode/node-nltools) are\nbeing merged into this project and will be maintained from here onward.\n\nWhile most of the algorithms are English-specific, contributors have implemented support for other languages. Thanks to Polyakov Vladimir, Russian stemming has been added! Thanks to David Przybilla, Spanish stemming has been added! Thanks to [even more contributors](https://github.com/NaturalNode/natural/graphs/contributors), stemming and tokenizing in more languages have been added.\n\nAside from this README, the only documentation is [this DZone article](http://www.dzone.com/links/r/using_natural_a_nlp_module_for_nodejs.html), [this course on Egghead.io](https://egghead.io/courses/natural-language-processing-in-javascript-with-natural), and [here on my blog](http://www.chrisumbel.com/article/node_js_natural_language_porter_stemmer_lancaster_bayes_naive_metaphone_soundex). The README is up to date, the other sources are somewhat outdated.\n\n### TABLE OF CONTENTS\n\n* [Installation](#installation)\n* [Tokenizers](#tokenizers)\n* [String Distance](#string-distance)\n* [Approximate String Matching](#approximate-string-matching)\n* [Stemmers](#stemmers)\n* [Classifiers](#classifiers)\n  * [Bayesian and logistic regression](#bayesian-and-logistic-regression)\n  * [Maximum Entropy Classifier](#maximum-entropy-classifier)\n* [Sentiment Analysis](#sentiment-analysis)\n* [Phonetics](#phonetics)\n* [Inflectors](#inflectors)\n* [N-Grams](#n-grams)\n* [tf-idf](#tf-idf)\n* [Tries](#tries)\n* [EdgeWeightedDigraph](#edgeweighteddigraph)\n* [ShortestPathTree](#shortestpathtree)\n* [LongestPathTree](#longestpathtree)\n* [WordNet](#wordnet)\n* [Spellcheck](#spellcheck)\n* [POS Tagger](#pos-tagger)\n* [Development](#development)\n* [License](#license)\n\n\n## Installation\n\nIf you're just looking to use natural without your own node application,\nyou can install via NPM like so:\n\n    npm install natural\n\nIf you're interested in contributing to natural, or just hacking on it, then by all\nmeans fork away!\n\n## Tokenizers\n\nWord, Regexp, and [Treebank tokenizers](ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html) are provided for breaking text up into\narrays of tokens:\n\n```javascript\nvar natural = require('natural');\nvar tokenizer = new natural.WordTokenizer();\nconsole.log(tokenizer.tokenize(\"your dog has fleas.\"));\n// [ 'your', 'dog', 'has', 'fleas' ]\n```\n\nThe other tokenizers follow a similar pattern:\n\n```javascript\ntokenizer = new natural.TreebankWordTokenizer();\nconsole.log(tokenizer.tokenize(\"my dog hasn't any fleas.\"));\n// [ 'my', 'dog', 'has', 'n\\'t', 'any', 'fleas', '.' ]\n\ntokenizer = new natural.RegexpTokenizer({pattern: /\\-/});\nconsole.log(tokenizer.tokenize(\"flea-dog\"));\n// [ 'flea', 'dog' ]\n\ntokenizer = new natural.WordPunctTokenizer();\nconsole.log(tokenizer.tokenize(\"my dog hasn't any fleas.\"));\n// [ 'my',  'dog',  'hasn',  '\\'',  't',  'any',  'fleas',  '.' ]\n\ntokenizer = new natural.OrthographyTokenizer({language: \"fi\"});\nconsole.log(tokenizer.tokenize(\"Mikä sinun nimesi on?\"));\n// [ 'Mikä', 'sinun', 'nimesi', 'on' ]\n```\n\nOverview of available tokenizers:\n\n| Tokenizer              | Language    | Explanation                                                             |\n|:-----------------------|:------------|:------------------------------------------------------------------------|\n| WordTokenizer          | Any         | Splits on anything except alphabetic characters, digits and underscore  |\n| WordPunctTokenizer     | Any         | Splits on anything except alphabetic characters, digits, punctuation and underscore  |\n| SentenceTokenizer      | Any         | Break string up in to parts based on punctation and quotation marks     |\n| CaseTokenizer          | Any?        | If lower and upper case are the same, the character is assumed to be whitespace or something else (punctuation) |\n| RegexpTokenizer        | Any         | Splits on a regular expression that either defines sequences of word characters or gap characters |\n| OrthographyTokenizer   | Finnish     | Splits on anything except alpabetic characters, digits and underscore   |\n| TreebankWordTokenizer  | Any         |  |\n| AggressiveTokenizer    | English     |  |\n| AggressiveTokenizerFa  | Farsi       |  |\n| AggressiveTokenizerFr  | French      |  |\n| AggressiveTokenizerRu  | Russian     |  |\n| AggressiveTokenizerEs  | Spanish     |  |\n| AggressiveTokenizerIt  | Italian     |  |\n| AggressiveTokenizerPl  | Polish      |  |\n| AggressiveTokenizerPt  | Portugese   |  |\n| AggressiveTokenizerNo  | Norwegian   |  |\n| AggressiveTokenizerSv  | Swedish     |  |\n| AggressiveTokenizerVi  | Vietnamese  |  |\n| TokenizerJa            | Japanese    |  |  |\n\n\n\n## String Distance\n\nNatural provides an implementation of three algorithms for calculating string distance: Hamming distance, Jaro-Winkler, Levenshtein distance, and Dice coefficient.\n\n[Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) measures the distance between two strings of equal length by counting the number of different characters. The third parameter indicates whether case should be ignored. By default the algorithm is case sensitive.\n```javascript\nvar natural = require('natural');\nconsole.log(natural.HammingDistance(\"karolin\", \"kathrin\", false));\nconsole.log(natural.HammingDistance(\"karolin\", \"kerstin\", false));\n// If strings differ in length -1 is returned\nconsole.log(natural.HammingDistance(\"short string\", \"longer string\", false));\n```\n\nOutput:\n```javascript\n3\n3\n-1\n```\n\n\nThe [Jaro–Winkler](http://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) string distance measuring algorithm will return a number between 0 and 1 which tells how closely the strings match (0 = not at all, 1 = exact match):\n\n```javascript\nvar natural = require('natural');\nconsole.log(natural.JaroWinklerDistance(\"dixon\",\"dicksonx\"));\nconsole.log(natural.JaroWinklerDistance('not', 'same'));\n```\n\nOutput:\n\n```javascript\n0.7466666666666666\n0\n```\n\nIf the distance between the strings is already known you can pass it as a third parameter. And you can force the algorithm to ignore case by passing a fourth parameter as follows:\n```javascript\nvar natural = require('natural');\nconsole.log(natural.JaroWinklerDistance(\"dixon\",\"dicksonx\", undefined, true));\n```\n\n\nNatural also offers support for [Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance) distances:\n\n```javascript\nvar natural = require('natural');\nconsole.log(natural.LevenshteinDistance(\"ones\",\"onez\"));\nconsole.log(natural.LevenshteinDistance('one', 'one'));\n```\n\nOutput:\n\n```javascript\n1\n0\n```\n\nThe cost of the three edit operations are modifiable for Levenshtein:\n\n```javascript\nconsole.log(natural.LevenshteinDistance(\"ones\",\"onez\", {\n    insertion_cost: 1,\n    deletion_cost: 1,\n    substitution_cost: 1\n}));\n```\n\nOutput:\n\n```javascript\n1\n```\n\nFull Damerau-Levenshtein matching can be used if you want to consider character transpositions as a valid edit operation.\n\n```javascript\nconsole.log(natural.DamerauLevenshteinDistance(\"az\", \"za\"));\n```\n\nOutput:\n```javascript\n1\n```\n\nThe transposition cost can be modified as well:\n\n```javascript\nconsole.log(natural.DamerauLevenshteinDistance(\"az\", \"za\", { transposition_cost: 0 }))\n```\n\nOutput:\n```javascript\n0\n```\n\nA restricted form of Damerau-Levenshtein (Optimal String Alignment) is available.\n\nThis form of matching is more space efficient than unrestricted Damerau-Levenshtein, by only considering a transposition if there are no characters between the transposed characters.\n\nComparison:\n\n```javascript\n// Optimal String Alignment\nconsole.log(natural.DamerauLevenshteinDistance('ABC', 'ACB'), { restricted: true });\n1\nconsole.log(natural.DamerauLevenshteinDistance('CA', 'ABC', { restricted: true }));\n2\n// Unrestricted Damerau-Levenshtein\nconsole.log(natural.DamerauLevenshteinDistance('CA', 'ABC', { restricted: false }));\n1\n```\n\nAnd [Dice's co-efficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient):\n\n```javascript\nvar natural = require('natural');\nconsole.log(natural.DiceCoefficient('thing', 'thing'));\nconsole.log(natural.DiceCoefficient('not', 'same'));\n```\n\nOutput:\n\n```javascript\n1\n0\n```\n\n## Approximate String Matching\nCurrently matching is supported via the Levenshtein algorithm.\n\n```javascript\nvar natural = require('natural');\nvar source = 'The RainCoat BookStore';\nvar target = 'All the best books are here at the Rain Coats Book Store';\n\nconsole.log(natural.LevenshteinDistance(source, target, {search: true}));\n```\n\nOutput:\n\n```javascript\n{ substring: 'the Rain Coats Book Store', distance: 4 }\n```\n\nThe following\n\n## Stemmers\n\nCurrently stemming is supported via the [Porter](http://tartarus.org/martin/PorterStemmer/index.html) and [Lancaster](http://www.comp.lancs.ac.uk/computing/research/stemming/) (Paice/Husk) algorithms. The Indonesian and Japanese stemmers do not follow a known algorithm.\n\n```javascript\nvar natural = require('natural');\n```\n\nThis example uses a Porter stemmer. \"word\" is returned.\n\n```javascript\nconsole.log(natural.PorterStemmer.stem(\"words\")); // stem a single word\n```\n\n in Russian:\n\n```javascript\nconsole.log(natural.PorterStemmerRu.stem(\"падший\"));\n```\n\n in Spanish:\n\n```javascript\nconsole.log(natural.PorterStemmerEs.stem(\"jugaría\"));\n```\n\nThe following stemmers are available:\n\n| Language      | Porter      | Lancaster | Other     | Module            |\n|:------------- |:-----------:|:---------:|:---------:|:------------------|\n| Dutch         | X           |           |           | `PorterStemmerNl` |\n| English       | X           |           |           | `PorterStemmer`   |\n| English       |             |  X        |           | `LancasterStemmer` |\n| Farsi (in progress) |  X    |           |           | `PorterStemmerFa` |\n| French        | X           |           |           | `PorterStemmerFr` |\n| Indonesian    |             |           | X         | `StemmerId`       |\n| Italian       | X           |           |           | `PorterStemmerIt` |\n| Japanese      |             |           | X         | `StemmerJa`       |\n| Norwegian     | X           |           |           | `PorterStemmerNo` |\n| Portugese     | X           |           |           | `PorterStemmerPt` |\n| Russian       | X           |           |           | `PorterStemmerRu` |\n| Swedish       | X           |           |           | `PorterStemmerSv` |\n\n\n`attach()` patches `stem()` and `tokenizeAndStem()` to String as a shortcut to\n`PorterStemmer.stem(token)`. `tokenizeAndStem()` breaks text up into single words\nand returns an array of stemmed tokens.\n\n```javascript\nnatural.PorterStemmer.attach();\nconsole.log(\"i am waking up to the sounds of chainsaws\".tokenizeAndStem());\nconsole.log(\"chainsaws\".stem());\n```\n\nThe same thing can be done with a Lancaster stemmer:\n\n```javascript\nnatural.LancasterStemmer.attach();\nconsole.log(\"i am waking up to the sounds of chainsaws\".tokenizeAndStem());\nconsole.log(\"chainsaws\".stem());\n```\n\n## Classifiers\n\n### Bayesian and logistic regression\n\nTwo classifiers are currently supported, [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) and [logistic regression](http://en.wikipedia.org/wiki/Logistic_regression).\nThe following examples use the BayesClassifier class, but the\nLogisticRegressionClassifier class could be substituted instead.\n\n```javascript\nvar natural = require('natural');\nvar classifier = new natural.BayesClassifier();\n```\n\nYou can train the classifier on sample text. It will use reasonable defaults to\ntokenize and stem the text.\n\n```javascript\nclassifier.addDocument('i am long qqqq', 'buy');\nclassifier.addDocument('buy the q\\'s', 'buy');\nclassifier.addDocument('short gold', 'sell');\nclassifier.addDocument('sell gold', 'sell');\n\nclassifier.train();\n```\n\nOutputs \"sell\"\n\n```javascript\nconsole.log(classifier.classify('i am short silver'));\n```\n\nOutputs \"buy\"\n\n```javascript\nconsole.log(classifier.classify('i am long copper'));\n```\n\nYou have access to the set of matched classes and the associated value from the classifier.\n\nOutputs:\n\n```javascript\n[ { label: 'buy', value: 0.39999999999999997 },\n  { label: 'sell', value: 0.19999999999999998 } ]\n```\n\nFrom this:\n\n```javascript\nconsole.log(classifier.getClassifications('i am long copper'));\n```\n\nThe classifier can also be trained with and can classify arrays of tokens, strings, or\nany mixture of the two. Arrays let you use entirely custom data with your own\ntokenization/stemming, if you choose to implement it.\n\n```javascript\nclassifier.addDocument(['sell', 'gold'], 'sell');\n```\n\nThe training process can be monitored by subscribing to the event `trainedWithDocument` that's emitted by the classifier, this event's emitted each time a document is finished being trained against:\n```javascript\n    classifier.events.on('trainedWithDocument', function (obj) {\n       console.log(obj);\n       /* {\n       *   total: 23 // There are 23 total documents being trained against\n       *   index: 12 // The index/number of the document that's just been trained against\n       *   doc: {...} // The document that has just been indexed\n       *  }\n       */\n    });\n```\nA classifier can also be persisted and recalled so you can reuse a training\n\n```javascript\nclassifier.save('classifier.json', function(err, classifier) {\n    // the classifier is saved to the classifier.json file!\n});\n```\n\nTo recall from the classifier.json saved above:\n\n```javascript\nnatural.BayesClassifier.load('classifier.json', null, function(err, classifier) {\n    console.log(classifier.classify('long SUNW'));\n    console.log(classifier.classify('short SUNW'));\n});\n```\n\nA classifier can also be serialized and deserialized like so:\n\n```javascript\nvar classifier = new natural.BayesClassifier();\nclassifier.addDocument(['sell', 'gold'], 'sell');\nclassifier.addDocument(['buy', 'silver'], 'buy');\n\n// serialize\nvar raw = JSON.stringify(classifier);\n// deserialize\nvar restoredClassifier = natural.BayesClassifier.restore(JSON.parse(raw));\nconsole.log(restoredClassifier.classify('i should sell that'));\n```\n\n__Note:__ if using the classifier for languages other than English you may need\nto pass in the stemmer to use. In fact, you can do this for any stemmer including\nalternate English stemmers. The default is the `PorterStemmer`.\n\n```javascript\nconst PorterStemmerRu = require('./node_modules/natural/lib/natural/stemmers/porter_stemmer_ru');\nvar classifier = new natural.BayesClassifier(PorterStemmerRu);\n```\n\n### Maximum Entropy Classifier\nThis module provides a classifier based on maximum entropy modelling. The central idea to maximum entropy modelling is to estimate a probability distribution that that has maximum entropy subject to the evidence that is available. This means that the distribution follows the data it has \"seen\" but does not make any assumptions beyond that.\n\nThe module is not specific to natural language processing, or any other application domain. There are little requirements with regard to the data structure it can be trained on. For training, it needs a sample that consists of elements. These elements have two parts:\n* part a: the class of the element\n* part b: the context of the element\nThe classifier will, once trained, return the most probable class for a particular context.\n\nWe start with an explanation of samples and elements. You have to create your own specialisation of the Element class. Your element class should implement the generateFeatures method for inferring feature functions from the sample.\n\n#### Samples and elements\nElements and contexts are created as follows:\n\n```javascript\nvar MyElement = require('MyElementClass');\nvar Context = require('Context');\nvar Sample = require('Sample');\n\nvar x = new MyElementClass(\"x\", new Context(\"0\"));\n// A sample is created from an array of elements\nvar sample = new Sample();\nsample.addElement(x);\n```\nA class is a string, contexts may be as complex as you want (as long as it can be serialised).\n\nA sample can be saved to and loaded from a file:\n```javascript\nsample.save('sample.json', function(error, sample) {\n  ...\n});\n```\nA sample can be read from a file as follows.\n\n```javascript\nsample.load('sample.json', MyElementClass, function(err, sample) {\n\n});\n```\nYou have to pass the element class to the load method so that the right element objects can be created from the data.\n\n#### Features and feature sets\nFeatures are functions that map elements to zero or one. Features are defined as follows:\n```javascript\nvar Feature = require('Feature');\n\nfunction f(x) {\n  if (x.b === \"0\") {\n    return 1;\n  }\n  return 0;\n}\n\nvar feature = new Feature(f, name, parameters);\n```\n<code>name</code> is a string for the name of the feature function, <code>parameters</code> is an array of strings for the parameters of the feature function. The combination of name and parameters should uniquely distinguish features from each other. Features that are added to a feature set are tested for uniqueness using these properties.\n\nA feature set is created like this\n```javascript\nvar FeatureSet = require('FeatureSet');\n\nvar set = new FeatureSet();\nset.addFeature(f, \"f\", [\"0\"]);\n```\n\nIn most cases you will generate feature functions using closures. For instance, when you generate feature functions in a loop that iterates through an array\n```javascript\nvar FeatureSet = require('FeatureSet');\nvar Feature = require('Feature');\n\nvar listOfTags = ['NN', 'DET', 'PREP', 'ADJ'];\nvar featureSet = new FeatureSet();\n\nlistofTags.forEach(function(tag) {\n  function isTag(x) {\n    if (x.b.data.tag === tag) {\n      return 1\n    }\n    return 0;\n  }\n  featureSet.addFeature(new Feature(isTag, \"isTag\", [tag]));\n});\n```\nIn this example you create feature functions that each have a different value for <code>tag</code> in their closure.\n\n#### Setting up and training the classifier\nA classifier needs the following parameter:\n* Classes: an array of classes (strings)\n* Features: an array of feature functions\n* Sample: a sample of elements for training the classifier\n\nA classifier can be created as follows:\n```javascript\nvar Classifier = require('Classifier');\nvar classifier = new Classifier(classes, featureSet, sample);\n```\nAnd it starts training with:\n```javascript\nvar maxIterations = 100;\nvar minImprovement = .01;\nvar p = classifier.train(maxIterations, minImprovement);\n```\nTraining is finished when either <code>maxIterations</code> is reached or the improvement in likelihood (of the sample) becomes smaller than <code>minImprovement</code>. It returns a probability distribution that can be stored and retrieved for later usage:\n```javascript\nclassifier.save('classifier.json', function(err, c) {\n  if (err) {\n    console.log(err);\n  }\n  else {\n    // Continue using the classifier\n  }\n});\n\nclassifier.load('classifier.json', function(err, c) {\n  if (err) {\n    console.log(err);\n  }\n  else {\n    // Use the classifier\n  }\n});\n```\n\nThe training algorithm is based on Generalised Iterative Scaling.\n\n#### Applying the classifier\nThe classifier can be used to classify contexts in two ways. To get the probabilities for all classes:\n```javascript\nvar classifications = classifier.getClassifications(context);\nclassifications.forEach(function(classPlusProbability) {\n  console.log('Class ' + classPlusProbability.label + ' has score ' + classPlusProbability.value);\n});\n```\nThis returns a map from classes to probabilities.\nTo get the highest scoring class:\n```javascript\nvar class = classifier.classify(context);\nconsole.log(class);\n```\n\n#### Simple example of maximum entropy modelling\nA  test is added to the spec folder based on simple elements that have contexts that are either \"0\" or \"1\", and classes are \"x\" and \"y\".\n```javascript\n{\n  \"a\": \"x\",\n  \"b\": {\n    \"data\": \"0\"\n  }\n}\n```\nIn the SE_Element class that inherits from Element, the method generateFeatures is implemented. It creates a feature function that tests for context \"0\".\n\nAfter setting up your own element class, the classifier can be created and trained.\n\n#### Application to POS tagging\nA more elaborate example of maximum entropy modelling is provided for part of speech tagging. The following steps are taken to create a classifier and apply it to a test set:\n* A new element class POS_Element is created that has a word window and a tag window around the word to be tagged.\n* From the Brown corpus a sample is generated consisting of POS elements.\n* Feature functions are generated from the sample.\n* A classifier is created and trained.\n* The classifier is applied to a test set. Results are compared to a simple lexicon-based tagger.  \n\n#### References\n* Adwait RatnaParkhi, Maximum Entropy Models For Natural Language Ambiguity Resolution, University of Pennsylvania, 1998, URL: http://repository.upenn.edu/cgi/viewcontent.cgi?article=1061&context=ircs_reports\n* Darroch, J.N.; Ratcliff, D. (1972). Generalized iterative scaling for log-linear models, The Annals of Mathematical Statistics, Institute of Mathematical Statistics, 43 (5): 1470–1480.\n\n## Sentiment Analysis\nThis is a simple sentiment analysis algorithm based on a vocabulary that assigns polarity to words. The algorithm calculates the sentiment of a piece of text by summing the polarity of each word and normalizing with the length of the sentence. If a negation occurs the result is made negative. It is used as follows:\n```javascript\nvar Analyzer = require('natural').SentimentAnalyzer;\nvar stemmer = require('natural').PorterStemmer;\nvar analyzer = new Analyzer(\"English\", stemmer, \"afinn\");\n// getSentiment expects an array of strings\nconsole.log(analyzer.getSentiment([\"I\", \"like\", \"cherries\"]));\n// 0.6666666666666666\n```\nThe constructor has three parameters:\n* Language: see below for supported languages.\n* Stemmer: to increase the coverage of the sentiment analyzer a stemmer may be provided. May be `null`.\n* Vocabulary: sets the type of vocabulary, `\"afinn\"`, `\"senticon\"` or `\"pattern\"` are valid values.\n\nCurrently, the following languages are supported with type of vocabulary and availability of negations (in alphabetic order):\n\n| Language      | AFINN       | Senticon  | Pattern   | Negations |\n| ------------- |:-----------:|:---------:|:---------:|:---------:|\n| Basque        |             |  X        |           |           |\n| Catalan       |             |  X        |           |           |\n| Dutch         |             |           | X         | X         |\n| English       | X           |  X        | X         | X         |\n| French        |             |           | X         |           |\n| Galician      |             |  X        |           |           |   \n| Italian       |             |           | X         |           |\n| Spanish       | X           |  X        |           | X         |     \n\nMore languages can be added by adding vocabularies and extending the map `languageFiles` in `SentimentAnalyzer.js`. In the tools folder below `lib/natural/sentiment` some tools are provided for transforming vocabularies in Senticon and Pattern format into a JSON format.\n\n\n\n### Acknowledgements and References\nThanks to Domingo Martín Mancera for providing the basis for this sentiment analyzer in his repo [Lorca](https://github.com/dmarman/lorca).\n\nAFINN is a list of English words rated for valence with an integer\nbetween minus five (negative) and plus five (positive). The words have\nbeen manually labeled by Finn Årup Nielsen in 2009-2011. Scientific reference can be found [here](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010). We used [afinn-165](https://github.com/words/afinn-165) which is available as nodejs module.\n\nThe senticon vocabulary is based on work by Fermin L. Cruz and others:\nCruz, Fermín L., José A. Troyano, Beatriz Pontes, F. Javier Ortega. Building layered, multilingual sentiment lexicons at synset and lemma levels, Expert Systems with Applications, 2014.\n\nThe Pattern vocabularies are from the [Pattern project](https://github.com/clips/pattern) of the CLiPS Research Center of University of Antwerpen. These have a PDDL license.\n\n## Phonetics\nPhonetic matching (sounds-like) matching can be done with the [SoundEx](http://en.wikipedia.org/wiki/Soundex),\n[Metaphone](http://en.wikipedia.org/wiki/Metaphone) or [DoubleMetaphone](http://en.wikipedia.org/wiki/Metaphone#Double_Metaphone) algorithms\n\n```javascript\nvar natural = require('natural');\nvar metaphone = natural.Metaphone;\nvar soundEx = natural.SoundEx;\n\nvar wordA = 'phonetics';\nvar wordB = 'fonetix';\n```\n\nTo test the two words to see if they sound alike:\n\n```javascript\nif(metaphone.compare(wordA, wordB))\n    console.log('they sound alike!');\n```\n\nThe raw phonetics are obtained with `process()`:\n\n```javascript\nconsole.log(metaphone.process('phonetics'));\n```\n\nA maximum code length can be supplied:\n\n```javascript\nconsole.log(metaphone.process('phonetics', 3));\n```\n\n`DoubleMetaphone` deals with two encodings returned in an array. This\nfeature is experimental and subject to change:\n\n```javascript\nvar natural = require('natural');\nvar dm = natural.DoubleMetaphone;\n\nvar encodings = dm.process('Matrix');\nconsole.log(encodings[0]);\nconsole.log(encodings[1]);\n```\n\nAttaching will patch String with useful methods:\n\n```javascript\nmetaphone.attach();\n```\n\n`soundsLike` is essentially a shortcut to `Metaphone.compare`:\n\n```javascript\nif(wordA.soundsLike(wordB))\n    console.log('they sound alike!');\n```\n\nThe raw phonetics are obtained with `phonetics()`:\n\n```javascript\nconsole.log('phonetics'.phonetics());\n```\n\nFull text strings can be tokenized into arrays of phonetics (much like how tokenization-to-arrays works for stemmers):\n\n```javascript\nconsole.log('phonetics rock'.tokenizeAndPhoneticize());\n```\n\nSame module operations applied with `SoundEx`:\n\n```javascript\nif(soundEx.compare(wordA, wordB))\n    console.log('they sound alike!');\n```\n\nThe same String patches apply with `soundEx`:\n\n```javascript\nsoundEx.attach();\n\nif(wordA.soundsLike(wordB))\n    console.log('they sound alike!');\n\nconsole.log('phonetics'.phonetics());\n```\n\n## Inflectors\n\n### Nouns\n\nNouns can be pluralized/singularized with a `NounInflector`:\n\n```javascript\nvar natural = require('natural');\nvar nounInflector = new natural.NounInflector();\n```\n\nTo pluralize a word (outputs \"radii\"):\n\n```javascript\nconsole.log(nounInflector.pluralize('radius'));\n```\n\nTo singularize a word (outputs \"beer\"):\n\n```javascript\nconsole.log(nounInflector.singularize('beers'));\n```\n\nLike many of the other features, String can be patched to perform the operations\ndirectly. The \"Noun\" suffix on the methods is necessary, as verbs will be\nsupported in the future.\n\n```javascript\nnounInflector.attach();\nconsole.log('radius'.pluralizeNoun());\nconsole.log('beers'.singularizeNoun());\n```\n\n### Numbers\n\nNumbers can be counted with a CountInflector:\n\n```javascript\nvar countInflector = natural.CountInflector;\n```\n\nOutputs \"1st\":\n\n```javascript\nconsole.log(countInflector.nth(1));\n```\n\nOutputs \"111th\":\n\n```javascript\nconsole.log(countInflector.nth(111));\n```\n\n### Present Tense Verbs\n\nPresent Tense Verbs can be pluralized/singularized with a PresentVerbInflector.\nThis feature is still experimental as of 0.0.42, so use with caution, and please\nprovide feedback.\n\n```javascript\nvar verbInflector = new natural.PresentVerbInflector();\n```\n\nOutputs \"becomes\":\n\n```javascript\nconsole.log(verbInflector.singularize('become'));\n```\n\nOutputs \"become\":\n\n```javascript\nconsole.log(verbInflector.pluralize('becomes'));\n```\n\nLike many other natural modules, `attach()` can be used to patch strings with\nhandy methods.\n\n```javascript\nverbInflector.attach();\nconsole.log('walk'.singularizePresentVerb());\nconsole.log('walks'.pluralizePresentVerb());\n```\n\n\n## N-Grams\n\nn-grams can be obtained for either arrays or strings (which will be tokenized\nfor you):\n\n```javascript\nvar NGrams = natural.NGrams;\n```\n\n### bigrams\n\n```javascript\nconsole.log(NGrams.bigrams('some words here'));\nconsole.log(NGrams.bigrams(['some',  'words',  'here']));\n```\n\nBoth of the above output: `[ [ 'some', 'words' ], [ 'words', 'here' ] ]`\n\n### trigrams\n\n```javascript\nconsole.log(NGrams.trigrams('some other words here'));\nconsole.log(NGrams.trigrams(['some',  'other', 'words',  'here']));\n```\n\nBoth of the above output: `[ [ 'some', 'other', 'words' ],\n  [ 'other', 'words', 'here' ] ]`\n\n### arbitrary n-grams\n\n```javascript\nconsole.log(NGrams.ngrams('some other words here for you', 4));\nconsole.log(NGrams.ngrams(['some', 'other', 'words', 'here', 'for',\n    'you'], 4));\n```\n\nThe above outputs: `[ [ 'some', 'other', 'words', 'here' ],\n  [ 'other', 'words', 'here', 'for' ],\n  [ 'words', 'here', 'for', 'you' ] ]`\n\n### padding\n\nn-grams can also be returned with left or right padding by passing a start and/or end symbol to the bigrams, trigrams or ngrams.\n\n```javascript\nconsole.log(NGrams.ngrams('some other words here for you', 4, '[start]', '[end]'));\n```\n\nThe above will output:\n```\n[ [ '[start]', '[start]', '[start]', 'some' ],\n  [ '[start]', '[start]', 'some', 'other' ],\n  [ '[start]', 'some', 'other', 'words' ],\n  [ 'some', 'other', 'words', 'here' ],\n  [ 'other', 'words', 'here', 'for' ],\n  [ 'words', 'here', 'for', 'you' ],\n  [ 'here', 'for', 'you', '[end]' ],\n  [ 'for', 'you', '[end]', '[end]' ],\n  [ 'you', '[end]', '[end]', '[end]' ] ]\n```\n\nFor only end symbols, pass `null` for the start symbol, for instance:\n```javascript\nconsole.log(NGrams.ngrams('some other words here for you', 4, null, '[end]'));\n```\n\nWill output:\n```\n[ [ 'some', 'other', 'words', 'here' ],\n  [ 'other', 'words', 'here', 'for' ],\n  [ 'words', 'here', 'for', 'you' ],\n  [ 'here', 'for', 'you', '[end]' ],\n  [ 'for', 'you', '[end]', '[end]' ],\n  [ 'you', '[end]', '[end]', '[end]' ] ]\n```\n\n### NGramsZH\n\nFor Chinese like languages, you can use NGramsZH to do a n-gram, and all apis are the same:\n\n```javascript\nvar NGramsZH = natural.NGramsZH;\nconsole.log(NGramsZH.bigrams('中文测试'));\nconsole.log(NGramsZH.bigrams(['中',  '文',  '测', '试']));\nconsole.log(NGramsZH.trigrams('中文测试'));\nconsole.log(NGramsZH.trigrams(['中',  '文', '测',  '试']));\nconsole.log(NGramsZH.ngrams('一个中文测试', 4));\nconsole.log(NGramsZH.ngrams(['一', '个', '中', '文', '测',\n    '试'], 4));\n```\n\n## tf-idf\n\n[Term Frequency–Inverse Document Frequency (tf-idf)](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) is implemented to determine how important a word (or words) is to a\ndocument relative to a corpus. The following formulas are used for calculating tf and idf:\n* tf(t, d) is a so-called raw count, so just the count of the term in the document\n* idf(t, D) uses the following formula: 1 + ln(N / (1 + n_t)) where N is the number of documents, and n_t the number of documents in which the term appears. The 1 + in the denominator is for handling the possibility that n_t is 0. \n\nThe following example will add four documents to\na corpus and determine the weight of the word \"node\" and then the weight of the\nword \"ruby\" in each document.\n\n```javascript\nvar natural = require('natural');\nvar TfIdf = natural.TfIdf;\nvar tfidf = new TfIdf();\n\ntfidf.addDocument('this document is about node.');\ntfidf.addDocument('this document is about ruby.');\ntfidf.addDocument('this document is about ruby and node.');\ntfidf.addDocument('this document is about node. it has node examples');\n\nconsole.log('node --------------------------------');\ntfidf.tfidfs('node', function(i, measure) {\n    console.log('document #' + i + ' is ' + measure);\n});\n\nconsole.log('ruby --------------------------------');\ntfidf.tfidfs('ruby', function(i, measure) {\n    console.log('document #' + i + ' is ' + measure);\n});\n```\n\nThe above outputs:\n\n```\nnode --------------------------------\ndocument #0 is 1\ndocument #1 is 0\ndocument #2 is 1\ndocument #3 is 2\nruby --------------------------------\ndocument #0 is 0\ndocument #1 is 1.2876820724517808\ndocument #2 is 1.2876820724517808\ndocument #3 is 0\n```\n\nThis approach can also be applied to individual documents.\n\nThe following example measures the term \"node\" in the first and second documents.\n\n```javascript\nconsole.log(tfidf.tfidf('node', 0));\nconsole.log(tfidf.tfidf('node', 1));\n```\n\nA TfIdf instance can also load documents from files on disk.\n\n```javascript\nvar tfidf = new TfIdf();\ntfidf.addFileSync('data_files/one.txt');\ntfidf.addFileSync('data_files/two.txt');\n```\n\nMultiple terms can be measured as well, with their weights being added into\na single measure value. The following example determines that the last document\nis the most relevant to the words \"node\" and \"ruby\".\n\n```javascript\nvar natural = require('natural');\nvar TfIdf = natural.TfIdf;\nvar tfidf = new TfIdf();\n\ntfidf.addDocument('this document is about node.');\ntfidf.addDocument('this document is about ruby.');\ntfidf.addDocument('this document is about ruby and node.');\n\ntfidf.tfidfs('node ruby', function(i, measure) {\n    console.log('document #' + i + ' is ' + measure);\n});\n```\n\nThe above outputs:\n\n```\ndocument #0 is 1\ndocument #1 is 1\ndocument #2 is 2\n```\n\nThe examples above all use strings, which causes natural to automatically tokenize the input.\nIf you wish to perform your own tokenization or other kinds of processing, you\ncan do so, then pass in the resultant arrays later. This approach allows you to bypass natural's\ndefault preprocessing.\n\n```javascript\nvar natural = require('natural');\nvar TfIdf = natural.TfIdf;\nvar tfidf = new TfIdf();\n\ntfidf.addDocument(['document', 'about', 'node']);\ntfidf.addDocument(['document', 'about', 'ruby']);\ntfidf.addDocument(['document', 'about', 'ruby', 'node']);\ntfidf.addDocument(['document', 'about', 'node', 'node', 'examples']);\n\ntfidf.tfidfs(['node', 'ruby'], function(i, measure) {\n    console.log('document #' + i + ' is ' + measure);\n});\n```\n\nIt's possible to retrieve a list of all terms in a document, sorted by their\nimportance.\n\n```javascript\ntfidf.listTerms(0 /*document index*/).forEach(function(item) {\n    console.log(item.term + ': ' + item.tfidf);\n});\n```\n\nA TfIdf instance can also be serialized and deserialized for save and recall.\n\n```javascript\nvar tfidf = new TfIdf();\ntfidf.addDocument('document one', 'un');\ntfidf.addDocument('document Two', 'deux');\nvar s = JSON.stringify(tfidf);\n// save \"s\" to disk, database or otherwise\n\n// assuming you pulled \"s\" back out of storage.\nvar tfidf = new TfIdf(JSON.parse(s));\n```\n\n## Tries\n\nTries are a very efficient data structure used for prefix-based searches.\nNatural comes packaged with a basic Trie implementation which can support match collection along a path,\nexistence search and prefix search.\n\n### Building The Trie\n\nYou need to add words to build up the dictionary of the Trie, this is an example of basic Trie set up:\n\n```javascript\nvar natural = require('natural');\nvar Trie = natural.Trie;\n\nvar trie = new Trie();\n\n// Add one string at a time\ntrie.addString(\"test\");\n\n// Or add many strings\ntrie.addStrings([\"string1\", \"string2\", \"string3\"]);\n```\n\n### Searching\n\n#### Contains\n\nThe most basic operation on a Trie is to see if a search string is marked as a word in the Trie.\n\n```javascript\nconsole.log(trie.contains(\"test\")); // true\nconsole.log(trie.contains(\"asdf\")); // false\n```\n\n### Find Prefix\n\nThe find prefix search will find the longest prefix that is identified as a word in the trie.\nIt will also return the remaining portion of the string which it was not able to match.\n\n```javascript\nconsole.log(trie.findPrefix(\"tester\"));     // ['test', 'er']\nconsole.log(trie.findPrefix(\"string4\"));    // [null, '4']\nconsole.log(trie.findPrefix(\"string3\"));    // ['string3', '']\n```\n\n### All Prefixes on Path\n\nThis search will return all prefix matches along the search string path.\n\n```javascript\ntrie.addString(\"tes\");\ntrie.addString(\"est\");\nconsole.log(trie.findMatchesOnPath(\"tester\")); // ['tes', 'test'];\n```\n\n### All Keys with Prefix\n\nThis search will return all of the words in the Trie with the given prefix, or [ ] if not found.\n\n```javascript\nconsole.log(trie.keysWithPrefix(\"string\")); // [\"string1\", \"string2\", \"string3\"]\n```\n\n### Case-Sensitivity\n\nBy default the trie is case-sensitive, you can use it in case-_in_sensitive mode by passing `false`\nto the Trie constructor.\n\n```javascript\ntrie.contains(\"TEST\"); // false\n\nvar ciTrie = new Trie(false);\nciTrie.addString(\"test\");\nciTrie.contains(\"TEsT\"); // true\n```\nIn the case of the searches which return strings, all strings returned will be in lower case if you are in case-_in_sensitive mode.\n\n## EdgeWeightedDigraph\n\nEdgeWeightedDigraph represents a digraph, you can add an edge, get the number vertexes, edges, get all edges and use toString to print the Digraph.\n\ninitialize a digraph:\n\n```javascript\nvar EdgeWeightedDigraph = natural.EdgeWeightedDigraph;\nvar digraph = new EdgeWeightedDigraph();\ndigraph.add(5,4,0.35);\ndigraph.add(5,1,0.32);\ndigraph.add(1,3,0.29);\ndigraph.add(6,2,0.40);\ndigraph.add(3,6,0.52);\ndigraph.add(6,4,0.93);\n```\nthe api used is: add(from, to, weight).\n\nget the number of vertexes:\n\n```javascript\nconsole.log(digraph.v());\n```\nyou will get 7.\n\nget the number of edges:\n\n```javascript\nconsole.log(digraph.e());\n```\nyou will get 6.\n\n\n\n## ShortestPathTree\n\nShortestPathTree represents a data type for solving the single-source shortest paths problem in\nedge-weighted directed acyclic graphs (DAGs).\nThe edge weights can be positive, negative, or zero. There are three APIs:\ngetDistTo(vertex),\nhasPathTo(vertex),\npathTo(vertex).\n\n```javascript\nvar ShortestPathTree = natural.ShortestPathTree;\nvar spt = new ShortestPathTree(digraph, 5);\n```\ndigraph is an instance of EdgeWeightedDigraph, the second param is the start vertex of DAG.\n\n### getDistTo(vertex)\n\nWill return the dist to vertex.\n\n```javascript\nconsole.log(spt.getDistTo(4));\n```\nthe output will be: 0.35\n\n### pathTo(vertex)\n\nWill return the shortest path:\n\n```javascript\nconsole.log(spt.pathTo(4));\n```\n\noutput will be:\n\n```javascript\n[5, 4]\n```\n\n## LongestPathTree\n\nLongestPathTree represents a data type for solving the single-source longest paths problem in\nedge-weighted directed acyclic graphs (DAGs).\nThe edge weights can be positive, negative, or zero. There are three APIs same as ShortestPathTree:\ngetDistTo(vertex),\nhasPathTo(vertex),\npathTo(vertex).\n\n```javascript\nvar LongestPathTree = natural.LongestPathTree;\nvar lpt = new LongestPathTree(digraph, 5);\n```\ndigraph is an instance of EdgeWeightedDigraph, the second param is the start vertex of DAG.\n\n### getDistTo(vertex)\n\nWill return the dist to vertex.\n\n```javascript\nconsole.log(lpt.getDistTo(4));\n```\nthe output will be: 2.06\n\n### pathTo(vertex)\n\nWill return the longest path:\n\n```javascript\nconsole.log(lpt.pathTo(4));\n```\n\noutput will be:\n\n```javascript\n[5, 1, 3, 6, 4]\n```\n\n## WordNet\n\nOne of the newest and most experimental features in natural is WordNet integration. Here's an\nexample of using natural to look up definitions of the word node. To use the WordNet module,\nfirst install the WordNet database files using [wordnet-db](https://github.com/moos/wordnet-db):\n\n    npm install wordnet-db\n\nKeep in mind that the WordNet integration is to be considered experimental at this point,\nand not production-ready. The API is also subject to change.  For an implementation with vastly increased performance, as well as a command-line interface, see [wordpos](https://github.com/moos/wordpos).\n\nHere's an example of looking up definitions for the word \"node\".\n\n```javascript\nvar wordnet = new natural.WordNet();\n\nwordnet.lookup('node', function(results) {\n    results.forEach(function(result) {\n        console.log('------------------------------------');\n        console.log(result.synsetOffset);\n        console.log(result.pos);\n        console.log(result.lemma);\n        console.log(result.synonyms);\n        console.log(result.pos);\n        console.log(result.gloss);\n    });\n});\n```\n\nGiven a synset offset and a part of speech, a definition can be looked up directly.\n\n```javascript\nvar wordnet = new natural.WordNet();\n\nwordnet.get(4424418, 'n', function(result) {\n    console.log('------------------------------------');\n    console.log(result.lemma);\n    console.log(result.pos);\n    console.log(result.gloss);\n    console.log(result.synonyms);\n});\n```\n\nIf you have _manually_ downloaded the WordNet database files, you can pass the folder to the constructor:\n\n```javascript\nvar wordnet = new natural.WordNet('/my/wordnet/dict');\n```\n\nAs of v0.1.11, WordNet data files are no longer automatically downloaded.\n\nPrinceton University \"About WordNet.\" WordNet. Princeton University. 2010. <http://wordnet.princeton.edu>\n\n## Spellcheck\n\nA probabilistic spellchecker based on http://norvig.com/spell-correct.html\n\nThis is best constructed with an array of tokens from a corpus, but a simple list of words from a dictionary will work.\n\n```javascript\nvar corpus = ['something', 'soothing'];\nvar spellcheck = new natural.Spellcheck(corpus);\n```\n\nIt uses the trie datastructure for fast boolean lookup of a word\n\n```javascript\nspellcheck.isCorrect('cat'); // false\n```\n\nIt suggests corrections (sorted by probability in descending order) that are up to a maximum edit distance away from the input word. According to Norvig, a max distance of 1 will cover 80% to 95% of spelling mistakes. After a distance of 2, it becomes very slow.\n\n```javascript\nspellcheck.getCorrections('soemthing', 1); // ['something']\nspellcheck.getCorrections('soemthing', 2); // ['something', 'soothing']\n```\n\n## POS Tagger\n\nThis is a part-of-speech tagger based on Eric Brill's transformational\nalgorithm. Transformation rules are specified in external files.\n\n### Usage\n```javascript\nvar natural = require(\"natural\");\nvar path = require(\"path\");\n\nvar base_folder = path.join(path.dirname(require.resolve(\"natural\")), \"brill_pos_tagger\");\nvar rulesFilename = base_folder + \"/data/English/tr_from_posjs.txt\";\nvar lexiconFilename = base_folder + \"/data/English/lexicon_from_posjs.json\";\nvar defaultCategory = 'N';\n\nvar lexicon = new natural.Lexicon(lexiconFilename, defaultCategory);\nvar rules = new natural.RuleSet(rulesFilename);\nvar tagger = new natural.BrillPOSTagger(lexicon, rules);\n\nvar sentence = [\"I\", \"see\", \"the\", \"man\", \"with\", \"the\", \"telescope\"];\nconsole.log(tagger.tag(sentence));\n```\nThis outputs the following:\n```\nSentence {\n  taggedWords:\n   [ { token: 'I', tag: 'NN' },\n     { token: 'see', tag: 'VB' },\n     { token: 'the', tag: 'DT' },\n     { token: 'man', tag: 'NN' },\n     { token: 'with', tag: 'IN' },\n     { token: 'the', tag: 'DT' },\n     { token: 'telescope', tag: 'NN' } ] }\n```\n\n### Lexicon\nThe lexicon is either a JSON file that has the following structure:\n```javascript\n{\n  \"word1\": [\"cat1\"],\n  \"word2\": [\"cat2\", \"cat3\"],\n  ...\n}\n```\nor a text file:\n```\nword1 cat1 cat2\nword2 cat3\n...\n```\nWords may have multiple categories in the lexicon file. The tagger uses only\nthe first category specified.\n\n### Specifying transformation rules\nTransformation rules are specified as follows:\n```\nOLD_CAT NEW_CAT PREDICATE PARAMETER\n```\nThis means that if the category of the current position is OLD_CAT and the predicate is true, the category is replaced by NEW_CAT. The predicate\nmay use the parameter in different ways: sometimes the parameter is used for\nspecifying the outcome of the predicate:\n```\nNN CD CURRENT-WORD-IS-NUMBER YES\n```\nThis means that if the outcome of predicate CURRENT-WORD-IS-NUMBER is YES, the\ncategory is replaced by <code>CD</code>.\nThe parameter can also be used to check the category of a word in the sentence:\n```\nVBD NN PREV-TAG DT\n```\nHere the category of the previous word must be <code>DT</code> for the rule to be applied.\n\n### Algorithm\nThe tagger applies transformation rules that may change the category of words. The input sentence is a Sentence object with tagged words. The tagged sentence is processed from left to right. At each step all rules are applied once; rules are applied in the order in which they are specified. Algorithm:\n```javascript\nBrill_POS_Tagger.prototype.applyRules = function(sentence) {\n  for (var i = 0, size = sentence.taggedWords.length; i < size; i++) {\n    this.ruleSet.getRules().forEach(function(rule) {\n      rule.apply(sentence, i);\n    });\n  }\n  return sentence;\n};\n```\nThe output is a Sentence object just like the input sentence.\n\n### Adding a predicate\nPredicates are defined in module <code>lib/RuleTemplates.js</code>. In that file\npredicate names are mapped to metadata for generaring transformation rules. The following properties must be supplied:\n* Name of the predicate\n* A function that evaluates the predicate (should return a boolean)\n* A window <code>[i, j]</code> that defines the span of the predicate in the\nsentence relative to the current position\n* The number of parameter the predicate needs: 0, 1 or 2\n* If relevant, a function for parameter 1 that returns its possible values\nat the current position in the sentence (for generating rules in training)\n* If relevant, a function for parameter 2 that returns its possible values\nat the current position in the sentence (for training)\n\nA typical entry for a rule templates looks like this:\n```javascript\n\"NEXT-TAG\": {\n    // maps to the predicate function\n    \"function\": next_tag_is,\n    // Minimum required window before or after current position to be a relevant predicate\n    \"window\": [0, 1],\n    // The number of parameters the predicate takes\n    \"nrParameters\": 1,\n    // Function that returns relevant values for parameter 1\n    \"parameter1Values\": nextTagParameterValues\n  }\n```\nA predicate function accepts a Sentence object, the current position in the\nsentence that should be tagged, and the outcome(s) of the predicate.\nAn example of a predicate that checks the category of the current word:\n```javascript\nfunction next_tag_is(sentence, i, parameter) {\n  if (i < sentence.taggedWords.length - 1) {\n    return(sentence.taggedWords[i + 1][1] === parameter);\n  }\n  else {\n    return(false);\n  }\n}\n```\n\nA values function for a parameter returns an array all possible parameter\nvalues given a location in a tagged sentence.\n```javascript\nfunction nextTagParameterValues(sentence, i) {\n  if (i < sentence.length - 1) {\n    return [sentence[i + 1].tag];\n  }\n  else {\n    return [];\n  }\n}\n```\n\n### Training\nThe trainer allows to learn a new set of transformation rules from a corpus.\nIt takes as input a tagged corpus and a set of rule templates. The algorithm\ngenerates positive rules (rules that apply at some location in the corpus)\nfrom the templates and iteratively extends and optimises the rule set.\n\nFirst, a corpus should be loaded. Currently, the format of Brown corpus is supported. Then a lexicon can be created from the corpus. The lexicon is needed for tagging the sentences before the learning algorithm is applied.\n```javascript\nvar natural = require(natural);\nvar text = fs.readFileSync(brownCorpusFile, 'utf8');\nvar corpus = new natural.Corpus(text, 1);\nvar lexicon = corpus.buildLexicon();\n```\nThe next step is to create a set of rule templates from which the learning\nalgorithm can generate transformation rules. Rule templates are defined in\n<code>PredicateMapping.js</code>.\n```javascript\nvar natural require('natural');\nvar templateNames = [\n  \"NEXT-TAG\",\n  \"NEXT-WORD-IS-CAP\",\n  \"PREV-1-OR-2-OR-3-TAG\",\n  \"...\",\n];\nvar templates = templateNames.map(function(name) {\n  return new natural.RuleTemplate(name);\n});\n```\nUsing lexicon and rule templates we can now start the trainer as follows.\n```javascript\nvar natural require('natural');\nvar Tester = require('natural.BrillPOSTrainer');\nvar trainer = new Trainer(/* optional threshold */);\nvar ruleSet = trainer.train(corpus, templates, lexicon);\n```\nA threshold value can be passed to constructor. Transformation rules with\na score below the threshold are removed after training.\nThe train method returns a set of transformation rules that can be used to\ncreate a POS tagger as usual. Also you can output the rule set in the right\nformat for later usage.\n```javascript\nconsole.log(ruleSet.prettyPrint());\n```\n\n### Testing\nNow we can apply the lexicon and rule set to a test set.\n```javascript\nvar tester = new natural.BrillPOSTester();\nvar tagger = new natural.BrillPOSTagger(lexicon, ruleSet);\nvar scores = tester.test(corpora[1], tagger);\n```\nThe test method returns an array of two percentages: first percentage is the ratio of right tags after tagging with the lexicon; second percentage is the ratio of right tags after applying the transformation rules.\n```javascript\nconsole.log(\"Test score lexicon \" + scores[0] + \"%\");\nconsole.log(\"Test score after applying rules \" + scores[1] + \"%\");\n```\n\n### Acknowledgements and References\n* Part of speech tagger by Percy Wegmann, https://code.google.com/p/jspos/\n* Node.js version of jspos: https://github.com/neopunisher/pos-js\n* A simple rule-based part of speech tagger, Eric Brill, Published in: Proceeding ANLC '92 Proceedings of the third conference on Applied natural language processing, Pages 152-155. http://dl.acm.org/citation.cfm?id=974526\n* Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging, Lance A. Ramshaw and Mitchell P. Marcus. http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/W/W94/W94-0111.pdf\n* Brown Corpus, https://en.wikipedia.org/wiki/Brown_Corpus\n\n## Development\n\nWhen developing, please:\n\n+ Write unit tests\n+ Make sure your unit tests pass\n\nThe current configuration of the unit tests requires the following environment variable to be set:\n\n    export NODE_PATH=.\n\n\n## License\n\nCopyright (c) 2011, 2012 Chris Umbel, Rob Ellis, Russell Mull\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\nWordNet License\n---------------\n\nThis license is available as the file LICENSE in any downloaded version of WordNet.\nWordNet 3.0 license: (Download)\n\nWordNet Release 3.0 This software and database is being provided to you, the LICENSEE, by Princeton University under the following license. By obtaining, using and/or copying this software and database, you agree that you have read, understood, and will comply with these terms and conditions.: Permission to use, copy, modify and distribute this software and database and its documentation for any purpose and without fee or royalty is hereby granted, provided that you agree to comply with the following copyright notice and statements, including the disclaimer, and that the same appear on ALL copies of the software, database and documentation, including modifications that you make for internal use or for distribution. WordNet 3.0 Copyright 2006 by Princeton University. All rights reserved. THIS SOFTWARE AND DATABASE IS PROVIDED \"AS IS\" AND PRINCETON UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT NOT LIMITATION, PRINCETON UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES OF MERCHANT- ABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR OTHER RIGHTS. The name of Princeton University or Princeton may not be used in advertising or publicity pertaining to distribution of the software and/or database. Title to copyright in this software, database and any associated documentation shall at all times remain with Princeton University and LICENSEE agrees to preserve same.\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git://github.com/NaturalNode/natural.git"
  },
  "scripts": {
    "benchmark": "node benchmarks",
    "clean": "rimraf *~ #* *classifier.json",
    "test": "NODE_PATH=. node_modules/jasmine-node/bin/jasmine-node spec/",
    "test_io": "NODE_PATH=. node_modules/jasmine-node/bin/jasmine-node io_spec/",
    "test_io_unclean": "NODE_PATH=. node_modules/jasmine-node/bin/jasmine-node io_spec/"
  },
  "version": "0.6.3"
}
